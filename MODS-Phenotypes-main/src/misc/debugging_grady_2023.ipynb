{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MISC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Debugging schema issues"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "for i in range(len(arrow_static_schema)):\n",
    "    try:\n",
    "        static_table = pq.read_table('/opt/bmi-585r/KLAB_SAIL/MODSPhenotypes/data/2022_07_30/extraction/emory/static_df/2014/',\n",
    "                                 schema=arrow_static_schema,\n",
    "                                 columns=[arrow_static_schema[i].name])\n",
    "    except:\n",
    "        print(i, arrow_static_schema[i].name)\n",
    "\n",
    "pq.read_table('/opt/bmi-585r/KLAB_SAIL/MODSPhenotypes/data/2022_07_30/extraction/emory/static_df/2014/',\n",
    "                                 schema=arrow_static_schema,\n",
    "                                 columns=['times_suspicion_sepsis3'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "numz = []\n",
    "typez = Counter()\n",
    "\n",
    "def get_stuffz(pickle_path):\n",
    "    encounter_pickle = load_pickle(pickle_path)\n",
    "    return get_typez(encounter_pickle), get_numz(encounter_pickle)\n",
    "\n",
    "def get_typez(encounter_pickle):\n",
    "    try:\n",
    "        return type(encounter_pickle['flags']['ed_wait_time'])\n",
    "    except KeyError:\n",
    "        return KeyError\n",
    "    \n",
    "def get_numz(encounter_pickle):\n",
    "    try:\n",
    "        return float(encounter_pickle['flags']['ed_wait_time'])\n",
    "    except KeyError:\n",
    "        return KeyError\n",
    "    \n",
    "with Pool(processes=num_cpus) as pool:\n",
    "    for typ, num in tqdm(\n",
    "        pool.imap(func=get_stuffz, iterable=pickle_paths), total=len(pickle_paths)\n",
    "    ):\n",
    "        typez[typ] += 1\n",
    "        numz.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(typez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sams = list(filter(lambda n: not math.isnan(n), list(filter(lambda n: (n != KeyError), numz))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sams).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numz=pd.Series(numz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numz.replace(KeyError, np.nan,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_pickle = load_pickle(pickle_paths[9413])\n",
    "ed_wait_time = encounter_pickle['flags']['ed_wait_time']\n",
    "ed_wait_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timedelta(569.633333333, unit='min').seconds/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(x) == pd.Timedelta:\n",
    "    return float(x.seconds/60)\n",
    "if type(x) == pd.NaT:\n",
    "    return float('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not currently used\n",
    "```python\n",
    "def cast_pandas_schema(df:pd.DataFrame, pandas_schema):\n",
    "    schema = {}\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            schema[col] = pandas_schema[col]\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    df = df.astype(schema)\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_pickle['abx_order_time']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_pickle['culture_times']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_pickle['sep3_time'].t_suspicion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " == np.sort(encounter_pickle['t_suspicion'].t_abx.array)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_pickle['sep3_time'].t_suspicion.array == encounter_pickle['t_suspicion'].t_suspicion.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.sort(encounter_pickle['t_suspicion'].t_abx.array)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.sort(encounter_pickle['abx_order_time'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.sort(encounter_pickle['t_suspicion'].t_abx.array) == np.sort(encounter_pickle['abx_order_time'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_pickle['culture_times']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[patient_id] = encounter_pickle[patient_id]\n",
    "df[service_id] = encounter_pickle[service_id]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sofa_df = encounter_pickle['sofa_scores']\n",
    "sofa_rename_map = {\n",
    "    'hourly_total': 'SOFA_hourly_total',\n",
    "    'delta_24h': 'SOFA_delta_24h',\n",
    "    'hourly_total_mod': 'SOFA_hourly_total_mod',\n",
    "    'delta_24h_mod': 'SOFA_delta_24h_mod'\n",
    "}\n",
    "sofa_df.rename(columns=sofa_rename_map, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sirs_df = encounter_pickle['sirs_scores']\n",
    "sirs_rename_map = {\n",
    "    'hourly_total': 'SIRS_hourly_total',\n",
    "    'delta_24h': 'SIRS_delta_24h'\n",
    "}\n",
    "sirs_df.rename(columns=sirs_rename_map, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dynamic_df = pd.merge(left=sofa_df, right=sirs_df,\n",
    "     how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dynamic_df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    int(encounter_pickle[patient_id])\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "    dynamic_df[patient_id] = int(encounter_pickle['flags'][patient_id])\n",
    "else:\n",
    "    dynamic_df[patient_id] = int(encounter_pickle[patient_id])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    int(encounter_pickle[service_id])\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "    dynamic_df[service_id] = int(encounter_pickle['flags'][service_id])\n",
    "else:\n",
    "    dynamic_df[service_id] = int(encounter_pickle[service_id])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dynamic_schema = {}\n",
    "for key in dynamic_keys:\n",
    "    dynamic_schema = dynamic_schema | pandas_schema['dynamic'][key]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dynamic_schema"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    dynamic_df = dynamic_df.astype(dynamic_schema)\n",
    "except KeyError as e:\n",
    "    # print(f\"KeyError: {e}\")\n",
    "    print(dynamic_schema)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "static_dict = reduce(lambda a, b: {**a, **b}, [encounter_pickle[k] for k in static_keys])\n",
    "static_df = pd.DataFrame(pd.Series(static_dict)).T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def extract_static_df(encounter_pickle, pickle_path):\n",
    "#    static_dict = reduce(lambda a, b: {**a, **b}, [encounter_pickle[k] for k in static_keys])\n",
    "#    static_df = pd.DataFrame(pd.Series(static_dict)).T\n",
    "    \n",
    "#     for key in static_keys:\n",
    "#         static_df = cast_pandas_schema(static_df, pandas_schema['static'][key])\n",
    "\n",
    "#     static_table = pa.Table.from_pandas(static_df, preserve_index=False)\n",
    "    \n",
    "#     output_folder = output_path / \"static_df\" / str(pickle_path.parent.stem)\n",
    "#     output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     pq.write_table(static_table,\n",
    "#                    output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "#                    # TODO: Get from config\n",
    "#                    version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "key = 'super_table'\n",
    "\n",
    "temp_df = encounter_pickle[key]\n",
    "for col in tqdm(temp_df.columns):\n",
    "    try:\n",
    "        temp_df[col].astype(pandas_mods_schema[col])\n",
    "    except TypeError as e:\n",
    "        print(f\"TypeError for col: {col} :: {e}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key in static_keys:\n",
    "    print(f\"**** {key} ****\")\n",
    "    temp_df = pd.DataFrame(pd.Series(encounter_pickle[key])).T\n",
    "\n",
    "    for col in tqdm(temp_df.columns):\n",
    "        print(col)\n",
    "        try:\n",
    "            temp_df[col].astype(pandas_mods_schema[col])\n",
    "        except TypeError as e:\n",
    "            print(f\"TypeError for col: {col} :: {e}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = encounter_pickle[dynamic_data_keys[0]]\n",
    "st = encounter_pickle['super_table']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for col in tmp.columns:\n",
    "    st[col] = [list(set(tmp[col].apply(pd.Series).stack().tolist()))] * len(st)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key in dynamic_keys:\n",
    "    print(len(encounter_pickle[key].columns))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key in dynamic_keys:\n",
    "    print(f\"**** {key} ****\")\n",
    "    temp_df = encounter_pickle[key]\n",
    "\n",
    "    for col in tqdm(temp_df.columns):\n",
    "        print(col)\n",
    "        try:\n",
    "            temp_df[col].astype(pandas_mods_schema[col])\n",
    "        except TypeError as e:\n",
    "            print(f\"TypeError for col: {col} :: {e}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def extract_dynamic_df(encounter_pickle, pickle_path):\n",
    "    sofa_df = encounter_pickle['sofa_scores']\n",
    "    sofa_rename_map = {\n",
    "        'hourly_total': 'SOFA_hourly_total',\n",
    "        'delta_24h': 'SOFA_delta_24h',\n",
    "        'hourly_total_mod': 'SOFA_hourly_total_mod',\n",
    "        'delta_24h_mod': 'SOFA_delta_24h_mod'\n",
    "    }\n",
    "    sofa_df.rename(columns=sofa_rename_map, inplace=True)\n",
    "\n",
    "    sirs_df = encounter_pickle['sirs_scores']\n",
    "    sirs_rename_map = {\n",
    "        'hourly_total': 'SIRS_hourly_total',\n",
    "        'delta_24h': 'SIRS_delta_24h'\n",
    "    }\n",
    "    sirs_df.rename(columns=sirs_rename_map, inplace=True)\n",
    "    \n",
    "    dynamic_df = pd.merge(left=sofa_df, right=sirs_df,\n",
    "         how='outer', left_index=True, right_index=True)\n",
    "\n",
    "    try:\n",
    "        int(encounter_pickle[patient_id])\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "        dynamic_df[patient_id] = int(encounter_pickle['flags'][patient_id])\n",
    "    else:\n",
    "        dynamic_df[patient_id] = int(encounter_pickle[patient_id])\n",
    "\n",
    "    try:\n",
    "        int(encounter_pickle[service_id])\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "        dynamic_df[service_id] = int(encounter_pickle['flags'][service_id])\n",
    "    else:\n",
    "        dynamic_df[service_id] = int(encounter_pickle[service_id])\n",
    "    \n",
    "    dynamic_schema = {}\n",
    "    for key in dynamic_keys:\n",
    "        dynamic_schema = dynamic_schema | pandas_schema['dynamic'][key]\n",
    "    try:\n",
    "        dynamic_df = dynamic_df.astype(dynamic_schema)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "        # print(dynamic_schema)\n",
    "\n",
    "    dynamic_df.reset_index(inplace=True, drop=False)\n",
    "    dynamic_df.rename(columns={\"index\": record_dt}, inplace=True)\n",
    "    \n",
    "    dynamic_table = pa.Table.from_pandas(dynamic_df, preserve_index=False)\n",
    "    \n",
    "    output_folder = output_path / \"dynamic_df\" / str(pickle_path.parent.stem)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pq.write_table(dynamic_table,\n",
    "                   output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "                   # TODO: Get from config\n",
    "                   version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# encounter_dfs = \n",
    "run_imap_multiprocessing(\n",
    "    process_encounter, __emory_pickle_paths__, num_cpus\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "__encounter_df__ = dd.concat(encounter_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_df[\"csn\"] = encounter_df[\"csn\"].astype(int)\n",
    "encounter_df[\"pt_id\"] = encounter_df[\"pt_id\"].astype(int)\n",
    "encounter_df[\"gender_code\"] = encounter_df[\"gender_code\"].astype(int)\n",
    "encounter_df[\"race_code\"] = encounter_df[\"race_code\"].astype(float)\n",
    "encounter_df[\"ethnicity_code\"] = encounter_df[\"ethnicity_code\"].astype(float)\n",
    "encounter_df[\"year\"] = encounter_df[\"hospital_admission_date_time\"].dt.year.astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = \"encounter\"\n",
    "fp = Path(config['emory']['filepaths']['extracted_data']) / f\"{name}_df\"\n",
    "fp.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_df.to_parquet(\n",
    "    path = fp / f\"{name}_df.parquet.snappy\",\n",
    "    compression = \"snappy\",\n",
    "    engine = \"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for yr in tqdm(years):  # TODO can I paralellize the exports?\n",
    "    df = encounter_df.query(f\"year == {yr}\").copy()\n",
    "    df.to_parquet(\n",
    "        path = fp / f\"{name}_df_{yr}.parquet.snappy\",\n",
    "        compression = \"snappy\",\n",
    "        engine = \"pyarrow\"\n",
    "    )\n",
    "    del(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "beds_dfs = [emory_pickle_data[i][1] for i in range(len(emory_pickle_data)) if not emory_pickle_data[i][1].empty]\n",
    "beds_df = pd.concat(beds_dfs, axis=0)\n",
    "beds_df.reset_index(inplace=True, drop=True)\n",
    "beds_df[\"year\"] = beds_df.bed_location_start.dt.year"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "beds_df['pat_id'] = beds_df['pat_id'].astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = \"beds\"\n",
    "fp = Path(config['emory']['filepaths']['extracted_data']) / f\"{name}_df\"\n",
    "fp.mkdir(parents=True, exist_ok=True)\n",
    "for yr in tqdm(years):  # TODO can I paralellize the exports?\n",
    "    df = beds_df.query(f\"year == {yr}\")\n",
    "    df.to_parquet(\n",
    "        path = fp / f\"{name}_df_{yr}.parquet.snappy\",\n",
    "        compression = \"snappy\",\n",
    "        engine = \"pyarrow\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "diagnosis_dfs = [emory_pickle_data[i][2] for i in range(len(emory_pickle_data)) if not emory_pickle_data[i][2].empty]\n",
    "diagnosis_df = pd.concat(diagnosis_dfs, axis=0)\n",
    "diagnosis_df.reset_index(inplace=True, drop=True)\n",
    "diagnosis_df['year'] = diagnosis_df.dx_time_date.dt.year"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "diagnosis_df['pat_id'] = diagnosis_df['pat_id'].astype(str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = \"diagnosis_df\"\n",
    "fp = Path(config['emory']['filepaths']['extracted_data']) / f\"{name}_df\"\n",
    "fp.mkdir(parents=True, exist_ok=True)\n",
    "for yr in tqdm(years):  # TODO can I paralellize the exports?\n",
    "    df = diagnosis_df.query(f\"year == {yr}\")\n",
    "    df.to_parquet(\n",
    "        path = fp / f\"{name}_df_{yr}.parquet.snappy\",\n",
    "        compression = \"snappy\",\n",
    "        engine = \"pyarrow\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "procedures_dfs = [emory_pickle_data[i][3] for i in range(len(emory_pickle_data)) if not emory_pickle_data[i][3].empty]\n",
    "procedures_df = pd.concat(procedures_dfs, axis=0)\n",
    "procedures_df.reset_index(inplace=True, drop=True)\n",
    "procedures_df['year'] = procedures_df.in_or_dttm.dt.year"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = \"procedures\"\n",
    "fp = Path(config['emory']['filepaths']['extracted_data']) / f\"{name}_df\"\n",
    "fp.mkdir(parents=True, exist_ok=True)\n",
    "for yr in tqdm(years):  # TODO can I paralellize the exports?\n",
    "    df = procedures_df.query(f\"year == {yr}\")\n",
    "    df.to_parquet(\n",
    "        path = fp / f\"{name}_df_{yr}.parquet.snappy\",\n",
    "        compression = \"snappy\",\n",
    "        engine = \"pyarrow\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cultures_dfs = [emory_pickle_data[i][4] for i in range(len(emory_pickle_data))]\n",
    "cultures_df = pd.concat(cultures_dfs, axis=0)\n",
    "cultures_df.reset_index(inplace=True, drop=True)\n",
    "cultures_df['year'] = cultures_df.specimen_collect_time.dt.year"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cultures_df['result_id'] = cultures_df['result_id'].replace('(null)', None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = \"cultures\"\n",
    "fp = Path(config['emory']['filepaths']['extracted_data']) / f\"{name}_df\"\n",
    "fp.mkdir(parents=True, exist_ok=True)\n",
    "for yr in tqdm(years):  # TODO can I paralellize the exports?\n",
    "    df = cultures_df.query(f\"year == {yr}\")\n",
    "    df.to_parquet(\n",
    "        path = fp / f\"{name}_df_{yr}.parquet.snappy\",\n",
    "        compression = \"snappy\",\n",
    "        engine = \"pyarrow\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extracted_data_fp = config['emory']['filepaths']['extracted_data']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls -lR $extracted_data_fp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import dask.dataframe as dd\n",
    "from tqdm.dask import TqdmCallback\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ray_path = Path(\"/opt/localdata/ray\")\n",
    "temp_dir = ray_path / \"temp\"\n",
    "temp_dir.mkdir(exist_ok=True, parents=True)\n",
    "import ray\n",
    "ray.init(\n",
    "    num_cpus=num_cpus,\n",
    "    num_gpus=0,\n",
    "    include_dashboard=False,\n",
    "    _temp_dir=str(temp_dir),\n",
    ")\n",
    "import modin.pandas as md"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols = pandas_schema.keys()\n",
    "\n",
    "for n in range(len(cols)):\n",
    "    try:\n",
    "        tbl = pq.read_table(output_path, schema=arrow_schema, columns=[list(cols)[n]])\n",
    "    except ArrowInvalid as e:\n",
    "        print(list(cols)[n])\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pkl1 = load_pickle(pickle_paths[4])\n",
    "# pkl2 = load_pickle(pickle_paths[6])\n",
    "pkl2 = load_pickle(pickle_paths[8])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pset1 = set(pkl1['super_table'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pset2 = set(pkl2['super_table'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pset1 - pset2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pset2 - pset1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for fp in tqdm(pickle_paths):\n",
    "    debug_processing(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p = multiprocessing.Pool(num_cpus)\n",
    "p.imap(debug_processing, pickle_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "manager = Manager()\n",
    "pool = Pool(processes=num_cpus)\n",
    "pool.map(func=debug_processing, iterable=pickle_paths)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmpdf = pd.DataFrame({\"datetime\": pd.date_range(\"2020-01-01T00:00:00Z\", freq=\"H\", periods=3)})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(tmpdf.dtypes)\n",
    "display(tmpdf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmptbl = pa.Table.from_pandas(tmpdf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmptbl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def empty_dataframe(k):\n",
    "    empty = {\n",
    "        't_suspicion': pd.DataFrame(data=[[pd.NaT,pd.NaT,pd.NaT]], columns=['t_abx','t_clt','t_suspicion']).drop(index=0),\n",
    "        'sep3_time': pd.DataFrame(data=[[pd.NaT,pd.NaT,pd.NaT]], columns=['t_suspicion','t_SOFA','t_sepsis3']).drop(index=0)\n",
    "    }\n",
    "    return empty[k]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "counter = Counter(t_suspicion=0,\n",
    " sofa_scores=0,\n",
    " sep3_time=0,\n",
    " sirs_scores=0,\n",
    " flags=0,\n",
    " event_times=0,\n",
    " static_features=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def check_empty_data(fp):\n",
    "    file = open(pickle_fp, \"rb\")\n",
    "    encounter_pickle = pickle.load(file, encoding=\"bytes\")\n",
    "    file.close()\n",
    "    empty = {}\n",
    "    for k in dynamic_data_keys:\n",
    "        empty[k] = int(encounter_pickle[k].empty)\n",
    "    for k in static_data_keys:\n",
    "        empty[k] = int(not any(encounter_pickle[k]))\n",
    "    # counter.update(empty)\n",
    "    return empty"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "empties = run_imap_multiprocessing(\n",
    "    check_empty_data, emory_pickle_paths, num_cpus\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for e in tqdm(empties):\n",
    "    counter.update(e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_df = None\n",
    "for i in range(len(pickle_paths)):\n",
    "    encounter_pickle = load_pickle(pickle_paths[i])\n",
    "    if any(encounter_pickle['culture_times']):\n",
    "        tmp_df = pd.DataFrame([[encounter_pickle['culture_times']]])\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pq.write_table(pa.Table.from_pandas(tmp_df, preserve_index=False),\n",
    "               f\"/home/gmatlin/tmp.parquet\",\n",
    "               version='2.6',\n",
    "               compression='snappy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "tmp_tbl = pq.read_table(f\"/home/gmatlin/tmp.parquet\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pa.schema({'0': 'timestamp[ns]'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_df = tmp_tbl.to_pandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_tbl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_df = encounter_pickle[\"super_table\"]\n",
    "tmp_df[patient_id] = int(encounter_pickle[patient_id])\n",
    "tmp_df[service_id] = int(encounter_pickle[service_id])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['norepinephrine_dose_unit'].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dtypes = {}\n",
    "for key in encounter.keys():\n",
    "    try:\n",
    "        dtypes[key] = encounter_pickle[key].convert_dtypes().dtypes\n",
    "    except AttributeError:\n",
    "        dtypes[key] = type(encounter_pickle[key])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if len(encounter_pickle['super_table']):\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(encounter_pickle['super_table'].convert_dtypes().dtypes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encounter_pickle['procedures_PerCSN'].dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if True:\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        display(pd.DataFrame(pd.Series(encounter_pickle['event_times'])).T.convert_dtypes().dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
