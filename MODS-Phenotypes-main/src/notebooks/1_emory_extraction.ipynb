{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "MODS Phenotypes: Step 1. Extract Data for Emory\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "from random import sample\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/opt/scratchspace/KLAB_SAIL/MODSPhenotypes/mods/\")\n",
    "from src.config import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_name = 'emory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = (\n",
    "    project_path / \"data\" / str(run_id) / \"extraction\" / site_name\n",
    ")\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = project_config[site_name][\"keys\"][\"patient_key\"]\n",
    "service_id = project_config[site_name][\"keys\"][\"service_key\"]\n",
    "record_dt = project_config[site_name][\"keys\"][\"record_dt\"]\n",
    "\n",
    "# TODO: this shouldnt be needed make it go away\n",
    "scores_keys = project_config[site_name][\"scores\"]\n",
    "static_keys = project_config[site_name][\"static\"]\n",
    "dynamic_keys = project_config[site_name][\"dynamic\"]\n",
    "times_keys = project_config[site_name][\"times\"]\n",
    "datetimes_keys = project_config[site_name][\"datetimes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction(pickle_path):\n",
    "    encounter_pickle, filename_without_ext = load_encounter_pickle(pickle_path)\n",
    "    extract_dynamic_df(encounter_pickle, pickle_path, filename_without_ext)\n",
    "    extract_static_df(encounter_pickle, pickle_path, filename_without_ext)\n",
    "    extract_perCSN_dfs(encounter_pickle, pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `extract_dynamic_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dynamic_df(encounter_pickle, pickle_path, filename_without_ext):\n",
    "    super_df = get_super_df(encounter_pickle, pickle_path, filename_without_ext)\n",
    "    \n",
    "    scores_df = get_scores_df(encounter_pickle, pickle_path)\n",
    "    \n",
    "    dynamic_df = pd.merge(left=super_df, right=scores_df,\n",
    "                          how='left', \n",
    "                        left_on=[record_dt], \n",
    "                        right_on=[record_dt],\n",
    "                       suffixes=('_super','_dynamic'))\n",
    "\n",
    "    dynamic_table = pa.Table.from_pandas(dynamic_df, preserve_index=False)\n",
    "    \n",
    "    output_folder = output_path / \"dynamic_df\" / str(pickle_path.parent.stem)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pq.write_table(dynamic_table,\n",
    "                   output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "                   # TODO: Get from config\n",
    "                   version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SuperTable"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def old__get_super_df(encounter_pickle, pickle_path, cols_to_drop = None):\n",
    "    # Get SuperTable\n",
    "    try:\n",
    "        super_df = encounter_pickle['super_table']\n",
    "        super_df[patient_id] = int(encounter_pickle[patient_id])\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in {str(pickle_path.stem)}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Drop columns\n",
    "    if cols_to_drop:\n",
    "        for col in cols_to_drop:\n",
    "            try:\n",
    "                super_df.drop(labels=col, axis=1, inplace=True)\n",
    "            except KeyError:\n",
    "                print(f\"KeyError in {str(pickle_path.stem)}: {e}\")\n",
    "                pass  # may not be in all encounters\n",
    "    \n",
    "    # Assign CSN to SuperTable\n",
    "    try:\n",
    "        int(encounter_pickle[service_id])\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in {str(pickle_path.stem)}: {e}\")\n",
    "        super_df[service_id] = int(encounter_pickle['flags'][service_id])\n",
    "    else:\n",
    "        super_df[service_id] = int(encounter_pickle[service_id])\n",
    "\n",
    "    super_df.reset_index(inplace=True, drop=False)\n",
    "    super_df.rename(columns={\"index\": record_dt}, inplace=True)\n",
    "    \n",
    "    for key in (set(pandas_schema['dynamic']['super_table'].keys()) - set(super_df.columns)): # TODO: Log these events\n",
    "        super_df[key] = None\n",
    "        super_df[key] = super_df[key].astype(pandas_schema['dynamic']['super_table'][key])\n",
    "\n",
    "    try:\n",
    "        super_df = super_df[pandas_schema['dynamic']['super_table'].keys()]\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: in {str(encounter_pickle[service_id])}: {e}\")\n",
    "\n",
    "    super_schema = {}\n",
    "    for col in super_df.columns:\n",
    "        try:\n",
    "            super_schema[col] = pandas_schema['dynamic']['super_table'][col]\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError in {str(pickle_path.stem)}: {e}\")\n",
    "    super_df = super_df.astype(super_schema)\n",
    "    \n",
    "    return super_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_super_df(encounter_pickle, pickle_path, filename_without_ext, cols_to_drop=None):\n",
    "    # Get SuperTable\n",
    "    try:\n",
    "        super_df = encounter_pickle['super_table']\n",
    "    except KeyError as e:\n",
    "        print(f\"(get_super_df) KeyError: 'super_table' not in {str(pickle_path.stem)}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        super_df[patient_id] = str(encounter_pickle['pt_id'])\n",
    "    except KeyError as e:\n",
    "        try:\n",
    "            super_df[patient_id] = str(encounter_pickle['pat_id'])\n",
    "        except KeyError as e:\n",
    "            print(f\"(get_super_df) KeyError: neither 'pt_id' or 'pat_id' was found in {str(pickle_path.stem)}\")\n",
    "            return\n",
    "\n",
    "    # Drop columns\n",
    "    if cols_to_drop:\n",
    "        for col in cols_to_drop:\n",
    "            try:\n",
    "                super_df.drop(labels=col, axis=1, inplace=True)\n",
    "            except KeyError:\n",
    "                print(f\"(get_super_df) KeyError in {str(pickle_path.stem)} when dropping '{col}' column\")\n",
    "                pass  # may not be in all encounters\n",
    "    \n",
    "    # Assign CSN to SuperTable\n",
    "    try:\n",
    "        str(encounter_pickle[service_id])\n",
    "    except KeyError as e:\n",
    "        super_df[service_id] = filename_without_ext  # Use the filename without extension here\n",
    "    else:\n",
    "        super_df[service_id] = str(encounter_pickle[service_id])\n",
    "\n",
    "    super_df.reset_index(inplace=True, drop=False)\n",
    "    super_df.rename(columns={\"index\": record_dt}, inplace=True)\n",
    "    \n",
    "    for key in (set(pandas_schema['dynamic']['super_table'].keys()) - set(super_df.columns)):\n",
    "        super_df[key] = None\n",
    "        super_df[key] = super_df[key].astype(pandas_schema['dynamic']['super_table'][key])\n",
    "\n",
    "    try:\n",
    "        super_df = super_df[pandas_schema['dynamic']['super_table'].keys()]\n",
    "    except KeyError as e:\n",
    "        print(f\"(get_super_df) KeyError in {str(encounter_pickle[service_id])} when using keys from pandas_schema: {e}\")\n",
    "\n",
    "    super_schema = {}\n",
    "    for col in super_df.columns:\n",
    "        try:\n",
    "            super_schema[col] = pandas_schema['dynamic']['super_table'][col]\n",
    "        except KeyError as e:\n",
    "            print(f\"(get_super_df) KeyError with {col} in {str(pickle_path.stem)} when building super_schema\")\n",
    "\n",
    "    try:\n",
    "        super_df = super_df.astype(super_schema)\n",
    "    except TypeError as e:\n",
    "        print(f\"TypeError in {str(pickle_path.stem)}: {e}\")\n",
    "        # Identify and print problematic column and its unique values\n",
    "        for col, dtype in super_schema.items():\n",
    "            try:\n",
    "                super_df[col].astype(dtype)\n",
    "            except Exception as inner_e:\n",
    "                print(f\"Column {col} with values {super_df[col].unique()} caused error: {inner_e}\")\n",
    "\n",
    "    return super_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_scores_df`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def old__get_scores_df(encounter_pickle, pickle_path):\n",
    "    sofa_df = encounter_pickle['sofa_scores']\n",
    "    sofa_rename_map = {\n",
    "        'hourly_total': 'SOFA_hourly_total',\n",
    "        'delta_24h': 'SOFA_delta_24h',\n",
    "        'hourly_total_mod': 'SOFA_hourly_total_mod',\n",
    "        'delta_24h_mod': 'SOFA_delta_24h_mod'\n",
    "    }\n",
    "    sofa_df.rename(columns=sofa_rename_map, inplace=True)\n",
    "\n",
    "    sirs_df = encounter_pickle['sirs_scores']\n",
    "    sirs_rename_map = {\n",
    "        'hourly_total': 'SIRS_hourly_total',\n",
    "        'delta_24h': 'SIRS_delta_24h'\n",
    "    }\n",
    "    sirs_df.rename(columns=sirs_rename_map, inplace=True)\n",
    "    \n",
    "    scores_df = pd.merge(left=sofa_df, right=sirs_df,\n",
    "         how='outer', left_index=True, right_index=True)\n",
    "\n",
    "    scores_schema = {}\n",
    "    for key in scores_keys:\n",
    "        scores_schema = scores_schema | pandas_schema['dynamic']['scores'][key]\n",
    "    try:\n",
    "        scores_df = scores_df.astype(scores_schema)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in {str(pickle_path.stem)}: {e}\")\n",
    "\n",
    "    scores_df.reset_index(inplace=True, drop=False)\n",
    "    scores_df.rename(columns={\"index\": record_dt}, inplace=True)\n",
    "    \n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_df(encounter_pickle, pickle_path):\n",
    "    # SOFA scores\n",
    "    sofa_df = encounter_pickle['sofa_scores']\n",
    "    sofa_rename_map = {\n",
    "        'hourly_total': 'SOFA_hourly_total',\n",
    "        'delta_24h': 'SOFA_delta_24h',\n",
    "        'hourly_total_mod': 'SOFA_hourly_total_mod',\n",
    "        'delta_24h_mod': 'SOFA_delta_24h_mod'\n",
    "    }\n",
    "    sofa_df.rename(columns=sofa_rename_map, inplace=True)\n",
    "\n",
    "    # Check for 'sirs_scores' in encounter_pickle\n",
    "    if 'sirs_scores' in encounter_pickle:\n",
    "        sirs_df = encounter_pickle['sirs_scores']\n",
    "        sirs_rename_map = {\n",
    "            'hourly_total': 'SIRS_hourly_total',\n",
    "            'delta_24h': 'SIRS_delta_24h'\n",
    "        }\n",
    "        sirs_df.rename(columns=sirs_rename_map, inplace=True)\n",
    "    else:\n",
    "        # Create an empty DataFrame with the same index as sofa_df\n",
    "        sirs_df = pd.DataFrame(index=sofa_df.index)\n",
    "    \n",
    "    # Merging\n",
    "    scores_df = pd.merge(left=sofa_df, right=sirs_df,\n",
    "                         how='outer', left_index=True, right_index=True)\n",
    "\n",
    "    # Type casting\n",
    "    scores_schema = {}\n",
    "    for key in scores_keys:\n",
    "        # Check if key exists in scores_df\n",
    "        if key in scores_df.columns:\n",
    "            scores_schema = scores_schema | pandas_schema['dynamic']['scores'][key]\n",
    "\n",
    "    # Check the existence of the column before trying to set its dtype\n",
    "    for col, dtype in scores_schema.items():\n",
    "        if col in scores_df.columns:\n",
    "            try:\n",
    "                scores_df[col] = scores_df[col].astype(dtype)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {str(pickle_path.stem)} for column {col}: {e}\")\n",
    "\n",
    "    scores_df.reset_index(inplace=True, drop=False)\n",
    "    scores_df.rename(columns={\"index\": record_dt}, inplace=True)\n",
    "    \n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `extract_static_df`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def old__extract_static_df(encounter_pickle, pickle_path):\n",
    "\n",
    "    static_df = get_static_df(encounter_pickle)\n",
    "    times_df = get_times_df(encounter_pickle)\n",
    "    \n",
    "    static_df = pd.concat([static_df, times_df], axis=1)\n",
    "\n",
    "    static_schema = {}\n",
    "    for key in static_keys:\n",
    "        static_schema = static_schema | pandas_schema['static'][key]\n",
    "    for key in times_keys:\n",
    "        static_schema = static_schema | pandas_schema['static']['times'][key]\n",
    "    try:\n",
    "        static_df = static_df.astype(static_schema)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in {str(pickle_path.stem)}: {e}\")\n",
    "\n",
    "    static_df.rename(columns={\n",
    "        't_suspicion': 'times_suspicion_sepsis3',\n",
    "        't_SOFA':'times_SOFA',\n",
    "        't_sepsis3':'times_sepsis3',\n",
    "        't_abx':'times_abx_order',\n",
    "        't_clt':'times_culture'\n",
    "        },\n",
    "                     inplace=True)\n",
    "    # dealing with `ed_wait_time`\n",
    "    # TODO: make this not awful\n",
    "    if type(static_df['ed_wait_time'][0]) is pd.Timedelta:\n",
    "        static_df['ed_wait_time'][0] = float(static_df['ed_wait_time'][0].seconds/60)\n",
    "\n",
    "    if pd.isnull(static_df['ed_wait_time'][0]):\n",
    "        static_df['ed_wait_time'][0] = 0.0\n",
    "        static_df['ed_wait_time'][0] = float('nan')\n",
    "            \n",
    "\n",
    "    static_table = pa.Table.from_pandas(static_df, preserve_index=False)\n",
    "\n",
    "    output_folder = output_path / \"static_df\" / str(pickle_path.parent.stem)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pq.write_table(static_table,\n",
    "                   output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "                   # TODO: Get from config\n",
    "                   version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_static_df(encounter_pickle, pickle_path, filename_without_ext):\n",
    "\n",
    "    static_df = get_static_df(encounter_pickle)\n",
    "    times_df = get_times_df(encounter_pickle)\n",
    "    \n",
    "    static_df = pd.concat([static_df, times_df], axis=1)\n",
    "\n",
    "    static_schema = {}\n",
    "    for key in static_keys:\n",
    "        static_schema = static_schema | pandas_schema['static'][key]\n",
    "    for key in times_keys:\n",
    "        static_schema = static_schema | pandas_schema['static']['times'][key]\n",
    "\n",
    "    # Filter the schema to only include columns present in the dataframe\n",
    "    filtered_schema = {col: dtype for col, dtype in static_schema.items() if col in static_df.columns}\n",
    "    try:\n",
    "        static_df = static_df.astype(filtered_schema)\n",
    "    except Exception as e:  # Catching a broader exception in case there are other issues beyond KeyError\n",
    "        print(f\"(extract_static_df) Error for {str(pickle_path.stem)} when doing .astype(filtered_schema): {e}\")\n",
    "\n",
    "    static_df.rename(columns={\n",
    "        't_suspicion': 'times_suspicion_sepsis3',\n",
    "        't_SOFA':'times_SOFA',\n",
    "        't_sepsis3':'times_sepsis3',\n",
    "        't_abx':'times_abx_order',\n",
    "        't_clt':'times_culture'\n",
    "        },\n",
    "                     inplace=True)\n",
    "    # dealing with `ed_wait_time`\n",
    "    if type(static_df.loc[0,'ed_wait_time']) is pd.Timedelta:\n",
    "        static_df.loc[0,'ed_wait_time'] = float(static_df['ed_wait_time'][0].seconds/60)\n",
    "\n",
    "    if pd.isnull(static_df.loc[0,'ed_wait_time']):\n",
    "        static_df.loc[0,'ed_wait_time'] = 0.0\n",
    "        static_df.loc[0,'ed_wait_time'] = float('nan')            \n",
    "\n",
    "    static_table = pa.Table.from_pandas(static_df, preserve_index=False)\n",
    "\n",
    "    output_folder = output_path / \"static_df\" / str(pickle_path.parent.stem)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pq.write_table(static_table,\n",
    "                   output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "                   # TODO: Get from config\n",
    "                   version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_static_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_df(encounter_pickle):\n",
    "    static_dict = reduce(lambda a, b: {**a, **b}, [encounter_pickle[k] for k in static_keys])\n",
    "\n",
    "    # Dealing with ed_wait_time issues\n",
    "    if 'ed_wait_time' not in static_dict.keys():\n",
    "        static_dict['ed_wait_time'] = float('nan')\n",
    "    \n",
    "    static_df = pd.DataFrame(pd.Series(static_dict)).T\n",
    "    return static_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_times_df`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def old__get_times_df(encounter_pickle):\n",
    "    times_data = [\n",
    "        denoise_times(encounter_pickle['sep3_time'].t_suspicion),\n",
    "        denoise_times(encounter_pickle['t_suspicion'].t_clt),\n",
    "        denoise_times(encounter_pickle['t_suspicion'].t_abx),\n",
    "        # denoise_times(encounter_pickle['t_suspicion'].t_suspicion), # Not used\n",
    "        # denoise_times(encounter_pickle['sep2_time'].t_suspicion), # Not used\n",
    "        # denoise_times(encounter_pickle['sep2_time'].t_SIRS), # Not used\n",
    "        # denoise_times(encounter_pickle['sep2_time'].t_sepsis2) # Not used\n",
    "        denoise_times(encounter_pickle['sep3_time'].t_SOFA),\n",
    "        denoise_times(encounter_pickle['sep3_time'].t_sepsis3)\n",
    "    ]\n",
    "\n",
    "    times_df = pd.DataFrame([times_data], columns = [\n",
    "        't_suspicion',\n",
    "        't_clt',\n",
    "        't_abx',\n",
    "        # 't_suspicion', # Not used\n",
    "        # 't_suspicion' # from 'sep2_time', Not used\n",
    "        # 't_SIRS', # Not used\n",
    "        # 't_sepsis2', # Not used\n",
    "        't_SOFA',\n",
    "        't_sepsis3'\n",
    "        ])\n",
    "    \n",
    "    for n in times_df.columns:\n",
    "        try:\n",
    "            if times_df[n][0].size == 0:\n",
    "                times_df[n][0] = [pd.NaT]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return times_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_times_df(encounter_pickle):\n",
    "    times_data = [\n",
    "        denoise_times(encounter_pickle['sep3_time'].t_suspicion),\n",
    "        denoise_times(encounter_pickle['t_suspicion'].t_clt),\n",
    "        denoise_times(encounter_pickle['t_suspicion'].t_abx),\n",
    "        denoise_times(encounter_pickle['sep3_time'].t_SOFA),\n",
    "        denoise_times(encounter_pickle['sep3_time'].t_sepsis3)\n",
    "    ]\n",
    "\n",
    "    times_df = pd.DataFrame([times_data], columns = [\n",
    "        't_suspicion',\n",
    "        't_clt',\n",
    "        't_abx',\n",
    "        't_SOFA',\n",
    "        't_sepsis3'\n",
    "        ])\n",
    "    \n",
    "    for n in times_df.columns:\n",
    "        try:\n",
    "            if times_df[n][0].size == 0:\n",
    "                times_df[n][0] = [pd.NaT]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return times_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `extract_perCSN_dfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_perCSN_dfs(encounter_pickle, pickle_path):\n",
    "    extract_beds_df(encounter_pickle, pickle_path)\n",
    "    extract_diagnosis_df(encounter_pickle, pickle_path)\n",
    "    extract_procedures_df(encounter_pickle, pickle_path)\n",
    "    extract_cultures_df(encounter_pickle, pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_beds_df(encounter_pickle, pickle_path):\n",
    "    beds_df = encounter_pickle[\"beds_PerCSN\"]\n",
    "    beds_df.reset_index(inplace=True)\n",
    "    beds_df.rename(columns={'index':'csn'}, inplace=True)\n",
    "\n",
    "    beds_table = pa.Table.from_pandas(beds_df, preserve_index=False)\n",
    "\n",
    "    output_folder = output_path / \"beds_df\" / str(pickle_path.parent.stem)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pq.write_table(beds_table,\n",
    "                   output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "                   version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_diagnosis_df(encounter_pickle, pickle_path):\n",
    "    diagnosis_df = encounter_pickle[\"diagnosis_PerCSN\"]\n",
    "    diagnosis_df.reset_index(inplace=True)\n",
    "    diagnosis_df.rename(columns={'index':'csn'}, inplace=True)\n",
    "\n",
    "    diagnosis_table = pa.Table.from_pandas(diagnosis_df, preserve_index=False)\n",
    "\n",
    "    output_folder = output_path / \"diagnosis_df\" / str(pickle_path.parent.stem)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pq.write_table(diagnosis_table,\n",
    "                   output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "                   version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_procedures_df(encounter_pickle, pickle_path):\n",
    "    procedures_df = encounter_pickle[\"procedures_PerCSN\"]\n",
    "    procedures_df.reset_index(inplace=True)\n",
    "    procedures_df.rename(columns={'index':'csn'}, inplace=True)\n",
    "\n",
    "    procedures_table = pa.Table.from_pandas(procedures_df, preserve_index=False)\n",
    "\n",
    "    output_folder = output_path / \"procedures_df\" / str(pickle_path.parent.stem)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pq.write_table(procedures_table,\n",
    "                   output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "                   version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cultures_df(encounter_pickle, pickle_path):\n",
    "    cultures_df = encounter_pickle[\"cultures_PerCSN\"]\n",
    "    cultures_df.reset_index(inplace=True)\n",
    "    cultures_df.rename(columns={'index':'csn'}, inplace=True)\n",
    "\n",
    "    cultures_table = pa.Table.from_pandas(cultures_df, preserve_index=False)\n",
    "\n",
    "    output_folder = output_path / \"cultures_df\" / str(pickle_path.parent.stem)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pq.write_table(cultures_table,\n",
    "                   output_folder / f\"{pickle_path.stem}.parquet\",\n",
    "                   version='2.6', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `main()`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def old__main(site_name='emory', sample_rate=1):\n",
    "    # bash: !rm -rf $output_path/*.parquet\n",
    "    pickle_paths = find_pickle_paths(site_name=site_name, sample_rate=sample_rate)\n",
    "    with Pool(processes=num_cpus) as pool:\n",
    "        max_ = len(pickle_paths)\n",
    "        with tqdm(total=max_) as pbar:\n",
    "            for _ in pool.imap_unordered(func=extraction, iterable=pickle_paths):\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e6214d1a624b9688b360eef210c345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35d5d65abe94bccae3a8fc9529b5d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/779373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(extract_static_df) Error for 17373465110 when doing .astype(filtered_schema): datetime64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 54472218018 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 46838348052 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 19654853365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 20340007365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 17067897365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 19703377363 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 54865797365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 32283227365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 2248567365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 55148007365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 1678267365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 13743938047 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 42379867365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 17964967364 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 17016197365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 45416024034 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 15169743365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 9800677365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 41709113365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 43431347362 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 19054844023 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 36444178032 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 34136287364 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 12465303365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 6149557364 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 8768277365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 4342384002 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 57386227365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 11322093365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 8154853365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 47769887365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 34809097365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 13564047365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 41550738016 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 16953857365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
      "(extract_static_df) Error for 46322397365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n"
     ]
    }
   ],
   "source": [
    "def main(config):\n",
    "    file_path=config[site_name]['filepaths']['encounter_pickles']\n",
    "    years=config[site_name]['years']\n",
    "    sample_rate=config['parameters']['sample_rate']\n",
    "    num_cpus=config['parameters']['num_cpus']\n",
    "    pickle_paths = find_pickle_paths(file_path=file_path,\n",
    "                                     years=years,\n",
    "                                     sample_rate=sample_rate)\n",
    "    with Pool(processes=num_cpus) as pool:\n",
    "        max_ = len(pickle_paths)\n",
    "        with tqdm(total=max_) as pbar:\n",
    "            for _ in pool.imap_unordered(func=extraction, iterable=pickle_paths):\n",
    "                pbar.update()\n",
    "\n",
    "main(project_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** TODO: SOLVE ERRORS WHEN EXTRACTING FROM STATIC_DF *** \n",
    "```text\n",
    "(extract_static_df) Error for 17373465110 when doing .astype(filtered_schema): datetime64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 54472218018 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 46838348052 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 19654853365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 20340007365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 17067897365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 19703377363 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 54865797365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 32283227365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 2248567365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 55148007365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 1678267365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 13743938047 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 42379867365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 17964967364 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 17016197365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 45416024034 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 15169743365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 9800677365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 41709113365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 43431347362 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 19054844023 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 36444178032 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 34136287364 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 12465303365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 6149557364 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 8768277365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 4342384002 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 57386227365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 11322093365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 8154853365 when doing .astype(filtered_schema): timedelta64[ns] cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 47769887365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 34809097365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 13564047365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 41550738016 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 16953857365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "(extract_static_df) Error for 46322397365 when doing .astype(filtered_schema): object cannot be converted to a FloatingDtype\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load `dynamic_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_csn = '10000548080'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "arrow_schema['perCSN']['cultures_PerCSN']['csn']='STRING'\n",
    "arrow_schema['perCSN']['cultures_PerCSN']['pat_id']='STRING'\n",
    "arrow_schema['perCSN']['cultures_PerCSN']['proc_code']='STRING'\n",
    "arrow_schema['perCSN']['cultures_PerCSN']['component_id']='STRING'\n",
    "arrow_schema['perCSN']['cultures_PerCSN']['order_id']='STRING'\n",
    "arrow_schema['perCSN']['cultures_PerCSN']['result_id']='STRING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_id</th>\n",
       "      <th>csn</th>\n",
       "      <th>charttime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>daily_weight_kg</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>sbp_line</th>\n",
       "      <th>dbp_line</th>\n",
       "      <th>map_line</th>\n",
       "      <th>sbp_cuff</th>\n",
       "      <th>...</th>\n",
       "      <th>SOFA_hourly_total</th>\n",
       "      <th>SOFA_delta_24h</th>\n",
       "      <th>SOFA_hourly_total_mod</th>\n",
       "      <th>SOFA_delta_24h_mod</th>\n",
       "      <th>SIRS_resp</th>\n",
       "      <th>SIRS_cardio</th>\n",
       "      <th>SIRS_temp</th>\n",
       "      <th>SIRS_wbc</th>\n",
       "      <th>SIRS_hourly_total</th>\n",
       "      <th>SIRS_delta_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20610789</td>\n",
       "      <td>10000548080</td>\n",
       "      <td>2018-04-17 05:14:00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>65.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20610789</td>\n",
       "      <td>10000548080</td>\n",
       "      <td>2018-04-17 06:14:00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>65.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20610789</td>\n",
       "      <td>10000548080</td>\n",
       "      <td>2018-04-17 07:14:00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>65.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20610789</td>\n",
       "      <td>10000548080</td>\n",
       "      <td>2018-04-17 08:14:00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>65.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20610789</td>\n",
       "      <td>10000548080</td>\n",
       "      <td>2018-04-17 09:14:00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>65.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pat_id          csn           charttime  temperature  daily_weight_kg  \\\n",
       "0  20610789  10000548080 2018-04-17 05:14:00         36.5             65.0   \n",
       "1  20610789  10000548080 2018-04-17 06:14:00         36.5             65.0   \n",
       "2  20610789  10000548080 2018-04-17 07:14:00         36.5             65.0   \n",
       "3  20610789  10000548080 2018-04-17 08:14:00         36.5             65.0   \n",
       "4  20610789  10000548080 2018-04-17 09:14:00         36.5             65.0   \n",
       "\n",
       "   height_cm  sbp_line  dbp_line  map_line  sbp_cuff  ...  SOFA_hourly_total  \\\n",
       "0      173.0       NaN       NaN       NaN     152.5  ...                0.0   \n",
       "1      173.0       NaN       NaN       NaN     152.5  ...                0.0   \n",
       "2      173.0       NaN       NaN       NaN     152.5  ...                0.0   \n",
       "3      173.0       NaN       NaN       NaN     152.5  ...                0.0   \n",
       "4      173.0       NaN       NaN       NaN     152.5  ...                0.0   \n",
       "\n",
       "   SOFA_delta_24h  SOFA_hourly_total_mod  SOFA_delta_24h_mod  SIRS_resp  \\\n",
       "0             0.0                    0.0                 0.0        0.0   \n",
       "1             0.0                    0.0                 0.0        0.0   \n",
       "2             0.0                    0.0                 0.0        0.0   \n",
       "3             0.0                    0.0                 0.0        0.0   \n",
       "4             0.0                    0.0                 0.0        0.0   \n",
       "\n",
       "   SIRS_cardio  SIRS_temp  SIRS_wbc  SIRS_hourly_total  SIRS_delta_24h  \n",
       "0          0.0        1.0       0.0                1.0             1.0  \n",
       "1          0.0        1.0       0.0                1.0             1.0  \n",
       "2          0.0        1.0       0.0                1.0             1.0  \n",
       "3          0.0        1.0       0.0                1.0             1.0  \n",
       "4          0.0        1.0       0.0                1.0             1.0  \n",
       "\n",
       "[5 rows x 124 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.7 ms, sys: 24.4 ms, total: 63.1 ms\n",
      "Wall time: 35.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dynamic_schema = (\n",
    "    arrow_schema['dynamic']['super_table'] |\n",
    "    reduce(lambda a, b: {**a, **b}, [arrow_schema['dynamic']['scores'][k] for k in scores_keys])\n",
    "    )\n",
    "\n",
    "arrow_dynamic_schema = make_arrow_schema(dynamic_schema)\n",
    "\n",
    "dynamic_table = pq.read_table(\n",
    "    str(output_path/'dynamic_df'/'2018'/f\"{example_csn}.parquet\"),\n",
    "    use_pandas_metadata=True,\n",
    "    schema=arrow_dynamic_schema\n",
    ")\n",
    "dynamic_df = dynamic_table.to_pandas()\n",
    "display(dynamic_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load `static_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csn</th>\n",
       "      <th>pt_id</th>\n",
       "      <th>y_vent_rows</th>\n",
       "      <th>y_vent_start_time</th>\n",
       "      <th>y_vent_end_time</th>\n",
       "      <th>vent_start_time</th>\n",
       "      <th>ed_wait_time</th>\n",
       "      <th>worst_pf_pa</th>\n",
       "      <th>worst_pf_pa_time</th>\n",
       "      <th>worst_pf_sp</th>\n",
       "      <th>...</th>\n",
       "      <th>hospital_admission_date_time</th>\n",
       "      <th>hospital_discharge_date_time</th>\n",
       "      <th>start_index</th>\n",
       "      <th>first_icu_start</th>\n",
       "      <th>first_icu_end</th>\n",
       "      <th>times_culture</th>\n",
       "      <th>times_abx_order</th>\n",
       "      <th>times_sepsis3</th>\n",
       "      <th>times_suspicion_sepsis3</th>\n",
       "      <th>times_SOFA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-04-17 05:14:00</td>\n",
       "      <td>2018-04-20 16:00:00</td>\n",
       "      <td>2018-04-17 05:14:00</td>\n",
       "      <td>2018-04-17 13:14:00</td>\n",
       "      <td>2018-04-18 15:14:00</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           csn     pt_id  y_vent_rows  y_vent_start_time  y_vent_end_time  \\\n",
       "0  10000548080  20610789          0.0                0.0              0.0   \n",
       "\n",
       "  vent_start_time  ed_wait_time  worst_pf_pa worst_pf_pa_time  worst_pf_sp  \\\n",
       "0             NaT           NaN          NaN              NaT          NaN   \n",
       "\n",
       "   ... hospital_admission_date_time hospital_discharge_date_time  \\\n",
       "0  ...          2018-04-17 05:14:00          2018-04-20 16:00:00   \n",
       "\n",
       "          start_index     first_icu_start       first_icu_end times_culture  \\\n",
       "0 2018-04-17 05:14:00 2018-04-17 13:14:00 2018-04-18 15:14:00        [None]   \n",
       "\n",
       "  times_abx_order times_sepsis3 times_suspicion_sepsis3 times_SOFA  \n",
       "0          [None]        [None]                  [None]     [None]  \n",
       "\n",
       "[1 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 ms, sys: 0 ns, total: 38.8 ms\n",
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "static_schema = (\n",
    "    reduce(lambda a, b: {**a, **b}, [arrow_schema['static'][k] for k in static_keys])\n",
    "    |\n",
    "    reduce(lambda a, b: {**a, **b}, [arrow_schema['static']['times'][k] for k in times_keys])\n",
    "    )\n",
    "\n",
    "# TODO: I shouldnt need to manually set these here figure out how to avoid it\n",
    "static_schema['times_abx_order'] = 'LIST(TIMESTAMP[NS])'\n",
    "static_schema['times_culture'] = 'LIST(TIMESTAMP[NS])'\n",
    "static_schema['times_suspicion_sepsis3'] = 'LIST(TIMESTAMP[NS])'\n",
    "static_schema['times_SOFA'] = 'LIST(TIMESTAMP[NS])'\n",
    "static_schema['times_sepsis3'] = 'LIST(TIMESTAMP[NS])'\n",
    "\n",
    "arrow_static_schema = make_arrow_schema(static_schema)\n",
    "\n",
    "static_table = pq.read_table(\n",
    "    str(output_path/'static_df'/'2018'/f\"{example_csn}.parquet\"),\n",
    "    use_pandas_metadata=True,\n",
    "    schema=arrow_static_schema\n",
    ")\n",
    "\n",
    "static_df = static_table.to_pandas()\n",
    "\n",
    "display(static_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load `beds_PerCSN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csn</th>\n",
       "      <th>pat_id</th>\n",
       "      <th>bed_location_start</th>\n",
       "      <th>bed_location_end</th>\n",
       "      <th>bed_unit</th>\n",
       "      <th>bed_room</th>\n",
       "      <th>bed_id</th>\n",
       "      <th>bed_label</th>\n",
       "      <th>hospital_service</th>\n",
       "      <th>accomodation_code</th>\n",
       "      <th>accomodation_description</th>\n",
       "      <th>icu</th>\n",
       "      <th>imc</th>\n",
       "      <th>ed</th>\n",
       "      <th>procedure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>2018-04-09 11:51:57</td>\n",
       "      <td>2018-04-17 05:13:56</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>THORACIC SURGERY</td>\n",
       "      <td>--</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>2018-04-17 05:13:56</td>\n",
       "      <td>2018-04-17 06:04:42</td>\n",
       "      <td>Main Registration ARR SJH</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>THORACIC SURGERY</td>\n",
       "      <td>--</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>2018-04-17 06:04:42</td>\n",
       "      <td>2018-04-17 13:09:59</td>\n",
       "      <td>SURG 1FL SJH</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>THORACIC SURGERY</td>\n",
       "      <td>--</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>2018-04-17 13:09:59</td>\n",
       "      <td>2018-04-18 15:51:36</td>\n",
       "      <td>2SW ICU SJH</td>\n",
       "      <td>295</td>\n",
       "      <td>01</td>\n",
       "      <td>INTENSIVE CARE</td>\n",
       "      <td>THORACIC SURGERY</td>\n",
       "      <td>ICU</td>\n",
       "      <td>INTENSIVE CARE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>2018-04-18 15:51:36</td>\n",
       "      <td>2018-04-20 16:00:00</td>\n",
       "      <td>3W SJH</td>\n",
       "      <td>349</td>\n",
       "      <td>01</td>\n",
       "      <td>SPECIAL CARE</td>\n",
       "      <td>THORACIC SURGERY</td>\n",
       "      <td>R</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           csn    pat_id  bed_location_start    bed_location_end  \\\n",
       "0  10000548080  20610789 2018-04-09 11:51:57 2018-04-17 05:13:56   \n",
       "1  10000548080  20610789 2018-04-17 05:13:56 2018-04-17 06:04:42   \n",
       "2  10000548080  20610789 2018-04-17 06:04:42 2018-04-17 13:09:59   \n",
       "3  10000548080  20610789 2018-04-17 13:09:59 2018-04-18 15:51:36   \n",
       "4  10000548080  20610789 2018-04-18 15:51:36 2018-04-20 16:00:00   \n",
       "\n",
       "                    bed_unit      bed_room        bed_id       bed_label  \\\n",
       "0               Not Recorded  Not Recorded  Not Recorded    Not Recorded   \n",
       "1  Main Registration ARR SJH  Not Recorded  Not Recorded    Not Recorded   \n",
       "2               SURG 1FL SJH  Not Recorded  Not Recorded    Not Recorded   \n",
       "3                2SW ICU SJH           295            01  INTENSIVE CARE   \n",
       "4                     3W SJH           349            01    SPECIAL CARE   \n",
       "\n",
       "   hospital_service accomodation_code accomodation_description  icu  imc   ed  \\\n",
       "0  THORACIC SURGERY                --             Not Recorded  0.0  0.0  0.0   \n",
       "1  THORACIC SURGERY                --             Not Recorded  0.0  0.0  0.0   \n",
       "2  THORACIC SURGERY                --             Not Recorded  0.0  0.0  0.0   \n",
       "3  THORACIC SURGERY               ICU           INTENSIVE CARE  1.0  0.0  0.0   \n",
       "4  THORACIC SURGERY                 R                  ROUTINE  0.0  0.0  0.0   \n",
       "\n",
       "   procedure  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 ms, sys: 0 ns, total: 23.9 ms\n",
      "Wall time: 18.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arrow_beds_schema = make_arrow_schema(arrow_schema['perCSN']['beds_PerCSN'])\n",
    "\n",
    "beds_table = pq.read_table(\n",
    "    str(output_path/'beds_df'/'2018'/f\"{example_csn}.parquet\"),\n",
    "    use_pandas_metadata=True,\n",
    "    schema=arrow_beds_schema\n",
    ")\n",
    "\n",
    "beds_df = beds_table.to_pandas()\n",
    "\n",
    "display(beds_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load `cultures_PerCSN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csn</th>\n",
       "      <th>pat_id</th>\n",
       "      <th>proc_code</th>\n",
       "      <th>proc_desc</th>\n",
       "      <th>component_id</th>\n",
       "      <th>component</th>\n",
       "      <th>loinc_code</th>\n",
       "      <th>specimen_collect_time</th>\n",
       "      <th>order_time</th>\n",
       "      <th>order_id</th>\n",
       "      <th>result_id</th>\n",
       "      <th>lab_result_time</th>\n",
       "      <th>result_status</th>\n",
       "      <th>lab_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [csn, pat_id, proc_code, proc_desc, component_id, component, loinc_code, specimen_collect_time, order_time, order_id, result_id, lab_result_time, result_status, lab_result]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 1.59 ms, total: 16 ms\n",
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "arrow_cultures_schema = make_arrow_schema(arrow_schema['perCSN']['cultures_PerCSN'])\n",
    "\n",
    "cultures_table = pq.read_table(\n",
    "    str(output_path/'cultures_df'/'2018'/f\"{example_csn}.parquet\"),\n",
    "    use_pandas_metadata=True,\n",
    "    schema=arrow_cultures_schema\n",
    ")\n",
    "\n",
    "cultures_df = cultures_table.to_pandas()\n",
    "\n",
    "display(cultures_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load `procedures_PerCSN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csn</th>\n",
       "      <th>pat_id</th>\n",
       "      <th>surgery_date</th>\n",
       "      <th>in_or_dttm</th>\n",
       "      <th>procedure_start_dttm</th>\n",
       "      <th>procedure_comp_dttm</th>\n",
       "      <th>out_or_dttm</th>\n",
       "      <th>or_procedure_id</th>\n",
       "      <th>primary_procedure_nm</th>\n",
       "      <th>cpt_code</th>\n",
       "      <th>service_nm</th>\n",
       "      <th>primary_physician_nm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [csn, pat_id, surgery_date, in_or_dttm, procedure_start_dttm, procedure_comp_dttm, out_or_dttm, or_procedure_id, primary_procedure_nm, cpt_code, service_nm, primary_physician_nm]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 0 ns, total: 14.8 ms\n",
      "Wall time: 12.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arrow_schema['perCSN']['procedures_PerCSN']['csn']='STRING'\n",
    "arrow_schema['perCSN']['procedures_PerCSN']['pat_id']='STRING'\n",
    "arrow_procedures_schema = make_arrow_schema(arrow_schema['perCSN']['procedures_PerCSN'])\n",
    "\n",
    "procedures_table = pq.read_table(\n",
    "    str(output_path/'procedures_df'/'2018'/f\"{example_csn}.parquet\"),\n",
    "    use_pandas_metadata=True,\n",
    "    schema=arrow_procedures_schema\n",
    ")\n",
    "\n",
    "procedures_df = procedures_table.to_pandas()\n",
    "\n",
    "display(procedures_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load `diagnosis_PerCSN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csn</th>\n",
       "      <th>pat_id</th>\n",
       "      <th>dx_line</th>\n",
       "      <th>dx_icd_scope</th>\n",
       "      <th>dx_code_icd9</th>\n",
       "      <th>dx_code_icd10</th>\n",
       "      <th>dx_source</th>\n",
       "      <th>dx_time_date</th>\n",
       "      <th>dx_code</th>\n",
       "      <th>dx_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Billing Diagnosis</td>\n",
       "      <td>401.9</td>\n",
       "      <td>I10</td>\n",
       "      <td>Medical Records Coding System</td>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>401.9</td>\n",
       "      <td>Unspecified essential hypertension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Admitting Diagnosis</td>\n",
       "      <td>424.0</td>\n",
       "      <td>I340</td>\n",
       "      <td>Saint Joseph's Hospital (HQ)</td>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>424.0</td>\n",
       "      <td>Mitral valve disorders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>Primary</td>\n",
       "      <td>Billing Diagnosis</td>\n",
       "      <td>424.0</td>\n",
       "      <td>I340</td>\n",
       "      <td>The Emory Clinic Registration (IDX)</td>\n",
       "      <td>2018-04-19</td>\n",
       "      <td>424.0</td>\n",
       "      <td>Mitral valve disorders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Billing Diagnosis</td>\n",
       "      <td>--</td>\n",
       "      <td>Z98890</td>\n",
       "      <td>The Emory Clinic Registration (IDX)</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>--</td>\n",
       "      <td>Not Recorded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000548080</td>\n",
       "      <td>20610789</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>Admitting Diagnosis</td>\n",
       "      <td>786.09</td>\n",
       "      <td>R0600</td>\n",
       "      <td>Medical Records Coding System</td>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>786.09</td>\n",
       "      <td>Other dyspnea and respiratory abnormality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           csn    pat_id       dx_line         dx_icd_scope dx_code_icd9  \\\n",
       "0  10000548080  20610789     Secondary    Billing Diagnosis        401.9   \n",
       "1  10000548080  20610789  Not Recorded  Admitting Diagnosis        424.0   \n",
       "2  10000548080  20610789       Primary    Billing Diagnosis        424.0   \n",
       "3  10000548080  20610789     Secondary    Billing Diagnosis           --   \n",
       "4  10000548080  20610789  Not Recorded  Admitting Diagnosis       786.09   \n",
       "\n",
       "  dx_code_icd10                            dx_source dx_time_date dx_code  \\\n",
       "0           I10        Medical Records Coding System   2018-04-24   401.9   \n",
       "1          I340         Saint Joseph's Hospital (HQ)   2018-03-21   424.0   \n",
       "2          I340  The Emory Clinic Registration (IDX)   2018-04-19   424.0   \n",
       "3        Z98890  The Emory Clinic Registration (IDX)   2018-04-18      --   \n",
       "4         R0600        Medical Records Coding System   2018-04-24  786.09   \n",
       "\n",
       "                                     dx_name  \n",
       "0         Unspecified essential hypertension  \n",
       "1                     Mitral valve disorders  \n",
       "2                     Mitral valve disorders  \n",
       "3                               Not Recorded  \n",
       "4  Other dyspnea and respiratory abnormality  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 ms, sys: 3.04 ms, total: 19.4 ms\n",
      "Wall time: 14.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arrow_schema['perCSN']['diagnosis_PerCSN']['csn']='STRING'\n",
    "arrow_schema['perCSN']['diagnosis_PerCSN']['pat_id']='STRING'\n",
    "arrow_diagnosis_schema = make_arrow_schema(arrow_schema['perCSN']['diagnosis_PerCSN'])\n",
    "\n",
    "diagnosis_table = pq.read_table(\n",
    "    str(output_path/'diagnosis_df'/'2018'/f\"{example_csn}.parquet\"),\n",
    "    use_pandas_metadata=True,\n",
    "    schema=arrow_diagnosis_schema\n",
    ")\n",
    "\n",
    "diagnosis_df = diagnosis_table.to_pandas()\n",
    "\n",
    "display(diagnosis_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mods",
   "language": "python",
   "name": "mods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
